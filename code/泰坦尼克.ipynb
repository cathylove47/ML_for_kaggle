{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 导入数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10f50f27261cce33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"author Binny\"\"\"\n",
    "# 忽略警告提示\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "# 导入处理数据包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(r'D:\\pythonProject1\\ML_for_kaggle\\data\\titanic\\train.csv')\n",
    "test_data = pd.read_csv(r'D:\\pythonProject1\\ML_for_kaggle\\data\\titanic\\test.csv')\n",
    "full = train_data.append(test_data, ignore_index=True)\n",
    "print('训练集大小：', train_data.shape, '测试集大小：', test_data.shape)\n",
    "print('合并后的数据集大小：', full.shape)\n",
    "print('合并后的数据集前5行：\\n', full.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27458e6dfead90e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "describe只能查看数据类型的描述统计信息，对于其他类型的数据不显示，比如字符串类型姓名（name），客舱号（Cabin）\n",
    "这很好理解，因为描述统计指标是计算数值，所以需要该列的数据类型是数据。\n",
    "Pandas Series.mean()函数返回给定Series对象中基础数据的平均值。\n",
    "'''\n",
    "# 获取数据类型列的描述统计信息\n",
    "full.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9676c23023fd8622"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a79ffc632706d388"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们发现数据一共有1309行,其中Survided只有891行,说明有418行数据没有标签,也就是我们要预测的数据\n",
    "Age(年龄)有1046行数据,说明有1309-1046=263行数据没有年龄,也就是说有263个乘客的年龄没有记录\n",
    "Fare(船票价格)有1308行数据,只有一个乘客没有记录\n",
    "Embarked(登船港口)有1307行数据,有两个乘客没有登船港口记录\n",
    "Cabin(客舱号)只有295行数据,有1007个乘客没有客舱号记录\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6035373ce77dacbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def PrintMissing(your_dataframe): \n",
    "    \"\"\"\n",
    "    打印缺失数据统计信息\n",
    "    :param 表示数据集\n",
    "    :return: 返回一个新的dataframe，包含三列：'缺失数据'、'缺失率'、'数据类型'\n",
    "    \"\"\"\n",
    "    drop_sum = your_dataframe.isnull().sum() # 统计每列缺失值个数\n",
    "    drop_sum_not_zero = drop_sum[drop_sum > 0] # 只保留有缺失值的列\n",
    "    df1 = pd.DataFrame({'缺失数据': drop_sum_not_zero}) # 创建一个新的dataframe，列名为'缺失数据'\n",
    "    df2 = pd.DataFrame({'缺失率': drop_sum_not_zero/your_dataframe.shape[0]}) # 创建一个新的dataframe，列名为'缺失率'\n",
    "    keys = drop_sum_not_zero.to_dict() # 将drop_sum_not_zero转换为字典\n",
    "    df_dtypes = pd.DataFrame({'数据类型': {key: your_dataframe[key].dtype for key in keys}}) # 创建一个新的dataframe，列名为'数据类型'\n",
    "    new_df = pd.concat([df1, df2.round(4), df_dtypes], axis=1)\n",
    "    return new_df\n",
    "\n",
    "print()\n",
    "print('缺失数据统计如下（总数 %d）：' % full.shape[0])\n",
    "PrintMissing(full)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c708c682fdd1777"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据清洗\n",
    "很多机器学习算法为了训练模型，要求所传入的特征中不能有空值。\n",
    "\n",
    "- 如果是数值类型，用平均值取代\n",
    "- 如果是分类数据，用最常见的类别取代\n",
    "- 使用模型预测缺失值，例如：K-NN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b355a6405c7282f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 年龄(Age)\n",
    "full['Age'] = full['Age'].fillna(full['Age'].mean())\n",
    "# 船票价格(Fare)\n",
    "full['Fare'] = full['Fare'].fillna(full['Fare'].mean())\n",
    "print()\n",
    "print('数值字段处理后：')\n",
    "PrintMissing(full)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12a186d89c533522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d70f3343d5826d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"让我们看看登船港口的数据长什么样子\"\"\"\n",
    "print('Embarked',list(pd.unique(full['Embarked'])))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbe0c3a9444ed424"
  },
  {
   "cell_type": "markdown",
   "source": [
    "出发地点：S=英国南安普顿Southampton\n",
    "途径地点1：C=法国 瑟堡市Cherbourg\n",
    "途径地点2：Q=爱尔兰 昆士敦Queenstown\n",
    "可以看出S港口的人数最多，占了72.4%，所以用S港口填充缺失值"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d34e8b6598977292"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "分类变量Embarked，看下最常见的类别，用其填充\n",
    "'''\n",
    "full['Embarked'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b8535dba9b03724"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full['Embarked'] = full['Embarked'].fillna('S')\n",
    "#船舱号（Cabin）：查看里面数据长啥样\n",
    "full[\"Cabin\"].head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2565626fa0321369"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#缺失数据比较多，船舱号（Cabin）缺失值填充为U，表示未知（Uknow） \n",
    "full[\"Cabin\"] = full[\"Cabin\"].fillna( 'U' )\n",
    "\n",
    "#检查数据处理是否正常\n",
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8a27b9253d4da20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#查看最终缺失值处理情况，记住生存情况（Survived）这里一列是我们的标签，用来做机器学习预测的，不需要处理这一列\n",
    "PrintMissing(full)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4109be6f550201e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征提取\n",
    "'''\n",
    "1.数值类型：\n",
    "    乘客编号（PassengerId），\n",
    "    年龄（Age），\n",
    "    船票价格（Fare），\n",
    "    同代直系亲属人数（SibSp），\n",
    "    不同代直系亲属人数（Parch）\n",
    "2.时间序列：无\n",
    "3.分类数据：\n",
    "    1）有直接类别的\n",
    "        乘客性别（Sex）：男性（male），女性（female）\n",
    "        登船港口（Embarked）：\n",
    "            出发地点：S=英国.南安普顿（Southampton），\n",
    "            途径地点1：C=法国.瑟堡市（Cherbourg），\n",
    "            出发地点2：Q=爱尔兰.昆士敦（Queenstown）\n",
    "        客舱等级（Pclass）：1=1等舱，2=2等舱，3=3等舱\n",
    "    2）字符串类型：可能从这里面提取出特征来，也归到分类数据中\n",
    "        乘客姓名（Name）\n",
    "        船舱号（Cabin）\n",
    "        船票编号（Ticket）\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d52f21dbde8908a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#查看性别数据这一列\n",
    "full[\"Sex\"].head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1a39bb3615657c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将性别的值映射为数值\n",
    "male为1,female为0\n",
    "\"\"\"\n",
    "# 如果为male就改为1，如果不是就改为0\n",
    "sex_mapDict = {u'male': 1, u'female': 0}\n",
    "full['Sex'] = full['Sex'].map(sex_mapDict)\n",
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98dbcc19525c7238"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#存放提取后的特征\n",
    "embarkedDf = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "使用get_dummies进行one-hot编码，产生虚拟变量（dummy variables），列名前缀是Embarked\n",
    "'''\n",
    "embarkedDf = pd.get_dummies( full[\"Embarked\"] , prefix=\"Embarked\" )\n",
    "embarkedDf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91fa157659d8292e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full\n",
    "full = pd.concat([full, embarkedDf], axis=1)\n",
    "\n",
    "'''\n",
    "因为已经使用登船港口(Embarked)进行了one-hot编码产生了它的虚拟变量（dummy variables）\n",
    "所以这里把登船港口(Embarked)删掉\n",
    "'''\n",
    "full.drop(\"Embarked\", axis=1, inplace=True)\n",
    "full.head()\n",
    "\n",
    "# 上面drop删除某一列代码解释：\n",
    "# 因为drop(name,axis=1)里面指定了name是哪一列，比如指定的是A这一列，axis=1表示按行操作。\n",
    "# 那么结合起来就是把A列里面每一行删除，最终结果是删除了A这一列.\n",
    "# 简单来说，使用drop删除某几列的方法记住这个语法就可以了：drop([列名1,列名2],axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cebbae1caf9e968"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "客舱等级(Pclass):\n",
    "1=1等舱，2=2等舱，3=3等舱\n",
    "'''\n",
    "# 存放提取后的特征\n",
    "pclassDf = pd.DataFrame()\n",
    "\n",
    "# 使用get_dummies进行one-hot编码，列名前缀是Pclass\n",
    "pclassDf = pd.get_dummies(full[\"Pclass\"], prefix=\"Pclass\")\n",
    "pclassDf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82f518c294b80098"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full\n",
    "full = pd.concat([full, pclassDf], axis=1)\n",
    "\n",
    "# 删掉客舱等级（Pclass）这一列\n",
    "full.drop(\"Pclass\", axis=1, inplace=True)\n",
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0963d23d553c62f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "查看姓名这一列长啥样\n",
    "注意到在乘客名字（Name）中，有一个非常显著的特点：\n",
    "乘客头衔每个名字当中都包含了具体的称谓或者说是头衔，将这部分信息提取出来后可以作为非常有用一个新变量，可以帮助我们进行预测。\n",
    "例如：\n",
    "    Braund, Mr. Owen Harris\n",
    "    Heikkinen, Miss. Laina\n",
    "    Oliva y Ocana, Dona. Fermina\n",
    "    Peter, Master. Michael J\n",
    "'''\n",
    "full[\"Name\"].head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863a47ecbe7519dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 练习从字符串中提取头衔，例如Mr\n",
    "# split用于字符串分割，返回一个列表\n",
    "# 我们看到姓名中'Braund, Mr. Owen Harris'，逗号前面的是“名”，逗号后面是‘头衔. 姓’\n",
    "name1 = 'Braund, Mr. Owen Harris'\n",
    "'''\n",
    "split用于字符串按分隔符分割，返回一个列表。这里按逗号分隔字符串\n",
    "也就是字符串'Braund, Mr. Owen Harris'被按分隔符,'拆分成两部分[Braund,Mr. Owen Harris]\n",
    "你可以把返回的列表打印出来瞧瞧，这里获取到列表中元素序号为1的元素，也就是获取到头衔所在的那部分，即Mr. Owen Harris这部分\n",
    "'''\n",
    "# Mr. Owen Harris\n",
    "str1 = name1.split(',')[1]\n",
    "'''\n",
    "继续对字符串Mr. Owen Harris按分隔符'.'拆分，得到这样一个列表[Mr, Owen Harris]\n",
    "这里获取到列表中元素序号为0的元素，也就是获取到头衔所在的那部分Mr\n",
    "'''\n",
    "# Mr.\n",
    "str2 = str1.split('.')[0]\n",
    "# strip() 方法用于移除字符串头尾指定的字符（默认为空格）\n",
    "str3 = str2.strip()\n",
    "print(str3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3e7a2f2e958538"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "定义函数：从姓名中获取头衔\n",
    "'''\n",
    "def getTitle(name):\n",
    "    str1 = name.split(',')[1]  # Mr. Owen Harris\n",
    "    str2 = str1.split('.')[0]  # Mr\n",
    "    # strip() 方法用于移除字符串头尾指定的字符（默认为空格）\n",
    "    str3 = str2.strip()\n",
    "    return str3\n",
    "# 存放提取后的特征\n",
    "titleDf = pd.DataFrame()\n",
    "# map函数：对Series每个数据应用自定义的函数计算\n",
    "titleDf[\"Name\"] = full[\"Name\"].map(getTitle)\n",
    "titleDf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d2d851758dfae5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "定义以下几种头衔类别：\n",
    "    Officer：政府官员\n",
    "    Royalty：王室（皇室）\n",
    "    Mr：已婚男士\n",
    "    Mrs：已婚妇女\n",
    "    Miss：年轻未婚女子\n",
    "    Master：有技能的人/教师\n",
    "'''\n",
    "# 姓名中头衔字符串与定义头衔类别的映射关系\n",
    "title_mapDict = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Royalty\",\n",
    "    \"Don\":        \"Royalty\",\n",
    "    \"Sir\":        \"Royalty\",\n",
    "    \"Dr\":         \"Officer\",\n",
    "    \"Rev\":        \"Officer\",\n",
    "    \"the Countess\": \"Royalty\",\n",
    "    \"Dona\":       \"Royalty\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\":         \"Mr\",\n",
    "    \"Mrs\":        \"Mrs\",\n",
    "    \"Miss\":       \"Miss\",\n",
    "    \"Master\":     \"Master\",\n",
    "    \"Lady\":       \"Royalty\"\n",
    "}\n",
    "# map函数：对Series每个数据应用自定义的函数计算\n",
    "titleDf[\"Title\"] = titleDf[\"Name\"].map(title_mapDict)\n",
    "titleDf.head()\n",
    "# 删除Name这一列\n",
    "titleDf.drop(\"Name\", axis=1, inplace=True)\n",
    "print(\"Title\", list(pd.unique(titleDf[\"Title\"])))\n",
    "# 使用get_dummies进行one-hot编码\n",
    "titleDf = pd.get_dummies(titleDf[\"Title\"])\n",
    "titleDf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da9ac9fc70ccdc1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full\n",
    "full = pd.concat([full, titleDf], axis=1)\n",
    "\n",
    "# 删掉姓名这一列\n",
    "full.drop(\"Name\", axis=1, inplace=True)\n",
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d05b7a41d5c4d191"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 存放客舱号信息\n",
    "cabinDf = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "客场号的类别值是首字母，例如：\n",
    "C85 类别映射为首字母C\n",
    "'''\n",
    "full['Cabin'] = full['Cabin'].map(lambda c: c[0])\n",
    "\n",
    "# 使用get_dummies进行one-hot编码，列名前缀是Cabin\n",
    "cabinDf = pd.get_dummies(full['Cabin'], prefix='Cabin')\n",
    "\n",
    "cabinDf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0f107ba34ba22cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full\n",
    "full = pd.concat([full, cabinDf], axis=1)\n",
    "\n",
    "# 删掉客舱号这一列\n",
    "full.drop(\"Cabin\", axis=1, inplace=True)\n",
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9899ac43ac1660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 存放家庭信息\n",
    "familyDf = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "家庭人数=同代直系亲属数（Parch）+不同代直系亲属数（SibSp）+乘客自己\n",
    "（因为乘客自己也是家庭成员的一个，所以这里加1）\n",
    "'''\n",
    "familyDf[\"FamilySize\"] = full[\"Parch\"] + full[\"SibSp\"] + 1\n",
    "\n",
    "'''\n",
    "家庭类别：\n",
    "    小家庭（Family_Single）：家庭人数=1\n",
    "    中等家庭（Family_Small）: 2<=家庭人数<=4\n",
    "    大家庭（Family_Large）: 家庭人数>=5\n",
    "'''\n",
    "# if 条件为真的时候返回if前面内容，否则返回0\n",
    "familyDf['Family_Single'] = familyDf[\"FamilySize\"].map(\n",
    "    lambda s: 1 if s == 1 else 0)\n",
    "familyDf['Family_Small'] = familyDf[\"FamilySize\"].map(\n",
    "    lambda s: 1 if 2 <= s <= 4 else 0)\n",
    "familyDf['Family_Large'] = familyDf[\"FamilySize\"].map(\n",
    "    lambda s: 1 if 5 <= s else 0)\n",
    "\n",
    "familyDf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d7149bc8ffbc42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full\n",
    "full = pd.concat([full, familyDf], axis=1)\n",
    "full.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a264636c701dd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#相关性矩阵\n",
    "corrDf = full.corr() \n",
    "corrDf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "696c15123310b012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "查看各个特征与生成情况（Survived）的相关系数，\n",
    "ascending=False表示按降序排列\n",
    "'''\n",
    "corrDf[\"Survived\"].sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13d592eb8ce36de9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征选择\n",
    "full_X = pd.concat([titleDf,  # 头衔\n",
    "                    pclassDf,  # 客舱等级\n",
    "                    familyDf,  # 家庭大小\n",
    "                    full[\"Fare\"],  # 船票价格\n",
    "                    cabinDf,  # 船舱号\n",
    "                    embarkedDf,  # 登船港口\n",
    "                    full[\"Sex\"]  # 性别\n",
    "                    ], axis=1)\n",
    "full_X.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "469f14257e5b7c72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "1）坦尼克号测试数据集因为是我们最后要提交给Kaggle的，里面没有生存情况的值，所以不能用于评估模型。\n",
    "    我们将Kaggle泰坦尼克号项目给我们的测试数据，叫做预测数据集（记为pred,也就是预测英文单词predict的缩写）。\n",
    "    也就是我们使用机器学习模型来对其生存情况就那些预测。\n",
    "2）我们使用Kaggle泰坦尼克号项目给的训练数据集，做为我们的原始数据集（记为source），\n",
    "    从这个原始数据集中拆分出训练数据集（记为train：用于模型训练）和测试数据集（记为test：用于模型评估）。\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "TRAIN_ROWS 是我们在最开始合并数据前知道的，原始数据集有总共有 TRAIN_ROWS 条数据\n",
    "从特征集合full_X中提取原始数据集提取前 TRAIN_ROWS 行数据时，我们要减去1，因为行号是从0开始的。\n",
    "'''\n",
    "TRAIN_ROWS = train_data.shape[0] # 训练数据集有多少行\n",
    "# 原始数据集：特征\n",
    "source_X = full_X.loc[0:TRAIN_ROWS-1, :]\n",
    "# 原始数据集：标签\n",
    "source_y = full.loc[0:TRAIN_ROWS-1, 'Survived']\n",
    "# 预测数据集：特征\n",
    "pred_X = full_X.loc[TRAIN_ROWS:, :]\n",
    "\n",
    "# 上面代码解释：\n",
    "# TRAIN_ROWS 行前面的数据是测试数据集，TRAIN_ROWS 行之后的数据是预测数据集。[TRAIN_ROWS:,:]就是\n",
    "# 从 TRAIN_ROWS 行开始到最后一行作为预测数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9fd0339a232c35f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "从原始数据集（source）中拆分出训练数据集（用于模型训练 train），测试数据集（用于模型评估 test）\n",
    "train_test_split 是交叉验证中常用的函数，功能是从样本中随机的按比例选取 train data 和 test data\n",
    "train_data：所要划分的样本特征集\n",
    "train_target：所要划分的样本结果\n",
    "test_size：样本占比，如果是整数的话就是样本的数量\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 建立模型用的训练数据集和测试数据集\n",
    "train_X, test_X, train_y, test_y = train_test_split(source_X,\n",
    "                                                    source_y,\n",
    "                                                    train_size=.8)\n",
    "\n",
    "# 输出数据集大小\n",
    "print('原始数据集特征：', source_X.shape,\n",
    "      '训练数据集特征：', train_X.shape,\n",
    "      '测试数据集特征：', test_X.shape)\n",
    "\n",
    "print('原始数据集标签：', source_y.shape,\n",
    "      '训练数据集标签：', train_y.shape,\n",
    "      '测试数据集标签：', test_y.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "482355c07665cecb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 原始数据查看\n",
    "source_y.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65fe40f518ab8b3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63936df0aaa50382"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_y.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea5dec7fb5ada6ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 第1步：导入算法\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 第2步：创建模型：逻辑回归（logisic regression）\n",
    "model = LogisticRegression(solver='liblinear', class_weight=\"balanced\")\n",
    "# 第3步：训练模型\n",
    "model.fit(train_X, train_y)\n",
    "# 分类问题，score得到的是模型的正确率\n",
    "model.score(test_X , test_y )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffd378c69db5e2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用机器学习模型，对预测数据集中的生存情况进行预测\n",
    "pred_Y = model.predict(pred_X)\n",
    "\n",
    "'''\n",
    "生成的预测值是浮点数（0.0,1,0）\n",
    "但是Kaggle要求提交的结果是整型（0,1）\n",
    "所以要对数据类型进行转换\n",
    "'''\n",
    "pred_Y = pred_Y.astype(int)\n",
    "# 乘客id\n",
    "passenger_id = full.loc[TRAIN_ROWS:, 'PassengerId']\n",
    "# 数据框：乘客id，预测生存情况的值\n",
    "predDf = pd.DataFrame(\n",
    "    {\"passenger_id\": passenger_id,\n",
    "     \"Survived\": pred_Y})\n",
    "predDf.shape\n",
    "predDf.head()\n",
    "# 保存结果\n",
    "predDf.to_csv('titanic_pred.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2967d763afe9af39"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# 将source_X和source_y进行拼接，得到原始数据集\n",
    "source = pd.concat([source_X, source_y], axis=1)\n",
    "# 将source分成训练数据集和测试数据集\n",
    "train_cathy, test_cathy = train_test_split(source, train_size=.8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T11:36:04.660404600Z",
     "start_time": "2023-09-03T11:36:04.567473300Z"
    }
   },
   "id": "e8261fedcfd13c0c"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictClass\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"agModels-predictClass\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.4\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.18363\n",
      "Disk Space Avail:   30.39 GB / 170.16 GB (17.9%)\n",
      "Train Data Rows:    712\n",
      "Train Data Columns: 27\n",
      "Label Column: Survived\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0.0, 1.0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19266.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.05 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 25 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  1 | ['Fare']\n",
      "\t\t('int', [])   : 26 | ['Master', 'Miss', 'Mr', 'Mrs', 'Officer', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Fare']\n",
      "\t\t('int', [])       :  1 | ['FamilySize']\n",
      "\t\t('int', ['bool']) : 25 | ['Master', 'Miss', 'Mr', 'Mrs', 'Officer', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t27 features in original data used to generate 27 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 569, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.6853\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6713\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7762\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7832\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 4: early stopping\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8811\t = Validation score   (accuracy)\n",
      "\t2.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.8811\t = Validation score   (accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 9.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictClass\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "label = 'Survived'\n",
    "save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
    "predictor = TabularPredictor(label=label, path=save_path,).fit(train_cathy)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T11:36:23.620555900Z",
     "start_time": "2023-09-03T11:36:13.613773700Z"
    }
   },
   "id": "6b3f13515ad187bc"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用测试集中的数据进行预测的情况为：\n",
      " 495    0.0\n",
      "648    0.0\n",
      "278    0.0\n",
      "31     1.0\n",
      "255    1.0\n",
      "      ... \n",
      "780    1.0\n",
      "837    0.0\n",
      "215    1.0\n",
      "833    0.0\n",
      "372    0.0\n",
      "Name: Survived, Length: 179, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictor.predict(test_cathy.drop(columns=[label]))\n",
    "print(\"用测试集中的数据进行预测的情况为：\\n\", y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T11:37:35.837916600Z",
     "start_time": "2023-09-03T11:37:35.752981300Z"
    }
   },
   "id": "e3ad3b867a87d1dd"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n0      RandomForestEntr    0.837989   0.790210        0.081233       0.073580   \n1      RandomForestGini    0.832402   0.790210        0.096945       0.061965   \n2        NeuralNetTorch    0.826816   0.881119        0.013992       0.007995   \n3   WeightedEnsemble_L2    0.826816   0.881119        0.016990       0.008995   \n4       NeuralNetFastAI    0.821229   0.874126        0.108938       0.010995   \n5            LightGBMXT    0.815642   0.874126        0.003995       0.003015   \n6              CatBoost    0.815642   0.867133        0.007996       0.003999   \n7        ExtraTreesGini    0.815642   0.776224        0.086776       0.063978   \n8              LightGBM    0.810056   0.874126        0.003997       0.002998   \n9         LightGBMLarge    0.810056   0.860140        0.003998       0.002999   \n10              XGBoost    0.810056   0.860140        0.017990       0.003998   \n11       ExtraTreesEntr    0.810056   0.783217        0.129925       0.069541   \n12       KNeighborsUnif    0.703911   0.685315        0.023965       0.024971   \n13       KNeighborsDist    0.703911   0.671329        0.023988       0.022971   \n\n    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0   0.556679                 0.081233                0.073580   \n1   0.576512                 0.096945                0.061965   \n2   2.794698                 0.013992                0.007995   \n3   3.583967                 0.002999                0.001000   \n4   0.712219                 0.108938                0.010995   \n5   0.458261                 0.003995                0.003015   \n6   0.786182                 0.007996                0.003999   \n7   0.553760                 0.086776                0.063978   \n8   0.623580                 0.003997                0.002998   \n9   0.574417                 0.003998                0.002999   \n10  0.339617                 0.017990                0.003998   \n11  0.544087                 0.129925                0.069541   \n12  0.005012                 0.023965                0.024971   \n13  0.005995                 0.023988                0.022971   \n\n    fit_time_marginal  stack_level  can_infer  fit_order  \n0            0.556679            1       True          6  \n1            0.576512            1       True          5  \n2            2.794698            1       True         12  \n3            0.789269            2       True         14  \n4            0.712219            1       True         10  \n5            0.458261            1       True          3  \n6            0.786182            1       True          7  \n7            0.553760            1       True          8  \n8            0.623580            1       True          4  \n9            0.574417            1       True         13  \n10           0.339617            1       True         11  \n11           0.544087            1       True          9  \n12           0.005012            1       True          1  \n13           0.005995            1       True          2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>score_val</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RandomForestEntr</td>\n      <td>0.837989</td>\n      <td>0.790210</td>\n      <td>0.081233</td>\n      <td>0.073580</td>\n      <td>0.556679</td>\n      <td>0.081233</td>\n      <td>0.073580</td>\n      <td>0.556679</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RandomForestGini</td>\n      <td>0.832402</td>\n      <td>0.790210</td>\n      <td>0.096945</td>\n      <td>0.061965</td>\n      <td>0.576512</td>\n      <td>0.096945</td>\n      <td>0.061965</td>\n      <td>0.576512</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NeuralNetTorch</td>\n      <td>0.826816</td>\n      <td>0.881119</td>\n      <td>0.013992</td>\n      <td>0.007995</td>\n      <td>2.794698</td>\n      <td>0.013992</td>\n      <td>0.007995</td>\n      <td>2.794698</td>\n      <td>1</td>\n      <td>True</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.826816</td>\n      <td>0.881119</td>\n      <td>0.016990</td>\n      <td>0.008995</td>\n      <td>3.583967</td>\n      <td>0.002999</td>\n      <td>0.001000</td>\n      <td>0.789269</td>\n      <td>2</td>\n      <td>True</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NeuralNetFastAI</td>\n      <td>0.821229</td>\n      <td>0.874126</td>\n      <td>0.108938</td>\n      <td>0.010995</td>\n      <td>0.712219</td>\n      <td>0.108938</td>\n      <td>0.010995</td>\n      <td>0.712219</td>\n      <td>1</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LightGBMXT</td>\n      <td>0.815642</td>\n      <td>0.874126</td>\n      <td>0.003995</td>\n      <td>0.003015</td>\n      <td>0.458261</td>\n      <td>0.003995</td>\n      <td>0.003015</td>\n      <td>0.458261</td>\n      <td>1</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CatBoost</td>\n      <td>0.815642</td>\n      <td>0.867133</td>\n      <td>0.007996</td>\n      <td>0.003999</td>\n      <td>0.786182</td>\n      <td>0.007996</td>\n      <td>0.003999</td>\n      <td>0.786182</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ExtraTreesGini</td>\n      <td>0.815642</td>\n      <td>0.776224</td>\n      <td>0.086776</td>\n      <td>0.063978</td>\n      <td>0.553760</td>\n      <td>0.086776</td>\n      <td>0.063978</td>\n      <td>0.553760</td>\n      <td>1</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>LightGBM</td>\n      <td>0.810056</td>\n      <td>0.874126</td>\n      <td>0.003997</td>\n      <td>0.002998</td>\n      <td>0.623580</td>\n      <td>0.003997</td>\n      <td>0.002998</td>\n      <td>0.623580</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>LightGBMLarge</td>\n      <td>0.810056</td>\n      <td>0.860140</td>\n      <td>0.003998</td>\n      <td>0.002999</td>\n      <td>0.574417</td>\n      <td>0.003998</td>\n      <td>0.002999</td>\n      <td>0.574417</td>\n      <td>1</td>\n      <td>True</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGBoost</td>\n      <td>0.810056</td>\n      <td>0.860140</td>\n      <td>0.017990</td>\n      <td>0.003998</td>\n      <td>0.339617</td>\n      <td>0.017990</td>\n      <td>0.003998</td>\n      <td>0.339617</td>\n      <td>1</td>\n      <td>True</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ExtraTreesEntr</td>\n      <td>0.810056</td>\n      <td>0.783217</td>\n      <td>0.129925</td>\n      <td>0.069541</td>\n      <td>0.544087</td>\n      <td>0.129925</td>\n      <td>0.069541</td>\n      <td>0.544087</td>\n      <td>1</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>KNeighborsUnif</td>\n      <td>0.703911</td>\n      <td>0.685315</td>\n      <td>0.023965</td>\n      <td>0.024971</td>\n      <td>0.005012</td>\n      <td>0.023965</td>\n      <td>0.024971</td>\n      <td>0.005012</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>KNeighborsDist</td>\n      <td>0.703911</td>\n      <td>0.671329</td>\n      <td>0.023988</td>\n      <td>0.022971</td>\n      <td>0.005995</td>\n      <td>0.023988</td>\n      <td>0.022971</td>\n      <td>0.005995</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_cathy, silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T11:44:40.268926Z",
     "start_time": "2023-09-03T11:44:39.588210700Z"
    }
   },
   "id": "a71d438f2b6fd58c"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.8268156424581006,\n 'balanced_accuracy': 0.7969696969696969,\n 'mcc': 0.6298122927747527,\n 'roc_auc': 0.8732542819499342,\n 'f1': 0.7479674796747967,\n 'precision': 0.8518518518518519,\n 'recall': 0.6666666666666666}"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_cathy, silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T11:45:58.567238500Z",
     "start_time": "2023-09-03T11:45:58.466724300Z"
    }
   },
   "id": "e55fb7033339128a"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "y_forecast = predictor.predict(pred_X)\n",
    "# 将y_forecast转化为dataframe\n",
    "y_forecast = pd.DataFrame(y_forecast)\n",
    "# 将y_forecast添加新的一列PassengerId,要求放在第一列\n",
    "y_forecast.insert(0, 'PassengerId', passenger_id)\n",
    "y_forecast\n",
    "# 保存为csv文件\n",
    "y_forecast.to_csv('titanic_pred_autogluon.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T12:04:18.359615100Z",
     "start_time": "2023-09-03T12:04:18.270667300Z"
    }
   },
   "id": "4b0eae97a345bbae"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# 将y_forecast保存为csv文件\n",
    "y_forecast.to_csv('titanic_pred_autogluon.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T11:54:11.751759200Z",
     "start_time": "2023-09-03T11:54:11.692796Z"
    }
   },
   "id": "ced9bf92fe3f9698"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "844da42170f97765"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
